{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SimWen\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\SimWen\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\SimWen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\SimWen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\SimWen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\SimWen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import contractions\n",
    "import spacy\n",
    "import nltk\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline, AutoModel\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# load require package\n",
    "\n",
    "import langchain\n",
    "from langchain.document_loaders import HuggingFaceDatasetLoader\n",
    "#from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from transformers import AutoModelForCausalLM, FalconForQuestionAnswering, FalconConfig\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "from langchain.vectorstores import VectorStore\n",
    "import pprint\n",
    "import random\n",
    "import string\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Ensure Spacy model is installed\n",
    "try:\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "except OSError:\n",
    "    from spacy.cli import download\n",
    "    download('en_core_web_sm')\n",
    "    nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to convert mixed case to lower case\n",
    "def convert_mixed_case_to_lower(x):\n",
    "    if isinstance(x, str):\n",
    "        lines = x.split('\\n')\n",
    "        new_lines = []\n",
    "        for line in lines:\n",
    "            words = line.split()\n",
    "            new_words = []\n",
    "            for word in words:\n",
    "                if word.isupper():\n",
    "                    new_words.append(word)\n",
    "                else:\n",
    "                    new_words.append(word.lower())\n",
    "            new_lines.append(' '.join(new_words))\n",
    "        return ' '.join(new_lines)\n",
    "    return x\n",
    "\n",
    "# Function to remove special character or ID patterns\n",
    "# def remove_special_ids(text, pattern=r'\\b\\w*\\d\\w*\\b'):\n",
    "#     if isinstance(text, str):\n",
    "#         return re.sub(pattern, ' ', text)\n",
    "#     return text\n",
    "\n",
    "def remove_punctuation(text):\n",
    "        text = re.sub(r'/', 'or', text)\n",
    "        text = re.sub(r'&', 'and', text)\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "        # text = re.sub(r'(?<!\\d)\\.(?!\\d)|[^\\w\\s.]', ' ', text)\n",
    "        return text\n",
    "\n",
    "def tokenize(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [word for word in tokens if word not in stop_words]\n",
    "\n",
    "def stemming(tokens):\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "def lemmatization(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "def expand_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "def remove_numbers(text):\n",
    "    return re.sub(r'\\d+', ' ', text)\n",
    "\n",
    "def remove_extra_whitespace(text):\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "# def remove_special_chars(df, columns):\n",
    "#     for col in columns:\n",
    "#         df[col] = df[col].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "#     return df\n",
    "\n",
    "def preprocess(text):\n",
    "    text = convert_mixed_case_to_lower(text)\n",
    "    # text = remove_special_ids(text)\n",
    "    text = expand_contractions(text)\n",
    "    text = remove_punctuation(text)\n",
    "    # text = remove_numbers(text)\n",
    "    # text = remove_special_chars(text)\n",
    "    text = remove_extra_whitespace(text)\n",
    "    tokens = tokenize(text)\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    tokens = lemmatization(tokens)  # or use stemming(tokens)\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Query For Embedding Searching, Raw Query for LLM Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "want scenario prevent failure\n"
     ]
    }
   ],
   "source": [
    "# Define your query\n",
    "#What is the detail of wrong wafer barcode scanner, and what are the action should be taken when facing this failure?\n",
    "query = \"i want a scenario that can prevent the failure\"\n",
    "preprocess_query = preprocess(query)\n",
    "print(preprocess_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Vectore Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load vectorstore.pkl file from local\n",
    "vectorstore_file = \"multi-qa-MiniLM-L6-cos-v1.pkl\"\n",
    "\n",
    "with open(vectorstore_file, \"rb\") as f:\n",
    "    global vectorstore\n",
    "    db: VectorStore = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SimWen\\AppData\\Local\\Temp\\ipykernel_14988\\649506487.py:11: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the path to the pre-trained model you want to use\n",
    "modelPath = \"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\"\n",
    "\n",
    "# Create a dictionary with model configuration options, specifying to use the CPU for computations\n",
    "model_kwargs = {'device':'cpu'}\n",
    "\n",
    "# Create a dictionary with encoding options, specifically setting 'normalize_embeddings' to False\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "\n",
    "# Initialize an instance of HuggingFaceEmbeddings with the specified parameters\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=modelPath,     # Provide the pre-trained model's path\n",
    "    model_kwargs=model_kwargs, # Pass the model configuration options\n",
    "    encode_kwargs=encode_kwargs, # Pass the encoding options\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the query and expand it\n",
    "def expand_query(query):\n",
    "    synonyms = set()\n",
    "    for word in query.split():\n",
    "        for syn in wordnet.synsets(word):\n",
    "            for lemma in syn.lemmas():\n",
    "                synonyms.add(lemma.name())\n",
    "    return query + \" \" + \" \".join(synonyms)\n",
    "\n",
    "def word_match_score(query, document):\n",
    "    query_words = set(query.lower().split())\n",
    "    doc_words = set(document.lower().split())\n",
    "    match_count = len(query_words.intersection(doc_words))\n",
    "    return match_count / len(query_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_Filtering(db, n_doc, query, k):\n",
    "    preprocess_query = preprocess(query)\n",
    "    # Extract document contents\n",
    "    all_docs = db.similarity_search(preprocess_query, k = n_doc)\n",
    "    doc_texts = [doc.page_content for doc in all_docs]\n",
    "\n",
    "    # Initialize the TF-IDF vectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Calculate TF-IDF matrix for all documents\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(doc_texts)\n",
    "\n",
    "    expanded_query = expand_query(preprocess_query)\n",
    "\n",
    "    # Calculate TF-IDF scores for the query\n",
    "    query_tfidf_vector = tfidf_vectorizer.transform([expanded_query])\n",
    "    \n",
    "    # Calculate cosine similarity between query TF-IDF vector and candidate TF-IDF vectors\n",
    "    cosine_similarities = cosine_similarity(tfidf_matrix, query_tfidf_vector).flatten()\n",
    "\n",
    "    # Calculate fuzzy matching scores\n",
    "    fuzzy_scores = [fuzz.partial_ratio(expanded_query, doc.page_content) / 100.0 for doc in all_docs]\n",
    "\n",
    "    # Calculate word matching scores\n",
    "    word_match_scores = [word_match_score(expanded_query, doc.page_content) for doc in all_docs]\n",
    "\n",
    "    # Combine the scores with additional word matching\n",
    "    combined_scores = [\n",
    "        ((0.5 * cos_sim + 0.8 * fuzz_score + 0.8 * word_match) / (0.5 + 0.8 + 0.8)) \n",
    "        for cos_sim, fuzz_score, word_match in zip(cosine_similarities, fuzzy_scores, word_match_scores)\n",
    "    ]\n",
    "\n",
    "    # Sort candidates based on TF-IDF cosine similarity\n",
    "    sorted_indices = sorted(range(len(combined_scores)), key=lambda i: combined_scores[i], reverse=True)\n",
    "    filtered_docs = [all_docs[i] for i in sorted_indices]\n",
    "    filtered_scores = [combined_scores[i] for i in sorted_indices]\n",
    "\n",
    "    return filtered_docs[:k], filtered_scores[:k], int(n_doc * 0.5)\n",
    "\n",
    "def filterEmbedding_db(filtered_docs, n_doc):\n",
    "    filter_db = FAISS.from_documents(filtered_docs[:n_doc], embeddings)\n",
    "    return filter_db, int(n_doc * 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Metadata:\n",
      "Similarity Score: 0.16965472960071526\n",
      "\n",
      "\n",
      "1 ) question: Failure cause of the work element : \n",
      "50.13.1.7 Destroy the wire for reject unit\n",
      "[Method_Z8K00014817]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1 ) Function of the process step and product characteristic: 50.0 Bonding station\n",
      "\n",
      "\n",
      "1 ) Failure effect FE to next higher level element and/or end user: Customer: Electrical/Functionality/Reliability Failure\n",
      "[Z8K00014817]\n",
      "IFX: Yield loss\n",
      "[Z8K00014817]\n",
      "\n",
      "\n",
      "1 ) Failure mode FM of the focus element: Stray Wire\n",
      "\n",
      "\n",
      "1 ) Current preventive control PC for FC: i) No destroy wire is require, use reject slip to identify reject unit slot, ink dot on reject die , and punch the lead during Dias inspection (P)\n",
      "\n",
      "iii) New design of Stray wire vacuum suction bin (P)\n",
      "\n",
      "\n",
      "1 ) Current detection control DC for FC or FM: ii) Visual Inspection - During 2hourly inspection / 3rd Optical inspection / Every change/replacement capillary / Every change device / Safe Release (D)\n",
      "\n",
      "\n",
      "1 ) subject: Unit Process FMEA for QFP+ Wire Bond Copper Wire\n",
      "\n",
      "\n",
      "1 ) package: QFP+\n",
      "\n",
      "\n",
      "1 ) location: Manufacturing-Melaka\n",
      "\n",
      "\n",
      "1 ) fmeaIid: Z8K00014817\n",
      "\n",
      "\n",
      "1 ) fmeaidx_id: Z8K00014817_77\n",
      "\n",
      "\n",
      "\n",
      "----------------------------\n",
      "\n",
      "\n",
      "Document Metadata:\n",
      "Similarity Score: 0.14483850845517204\n",
      "\n",
      "\n",
      "2 ) question: Failure cause of the work element : \n",
      "50.1.7.3 Die Surface Contamination\n",
      "[Material_Z8K00014817]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2 ) Function of the process step and product characteristic: 50.0 Bonding station\n",
      "\n",
      "\n",
      "2 ) Failure effect FE to next higher level element and/or end user: Customer: Electrical/Functionality/Reliability Failure\n",
      "[Z8K00014817]\n",
      "IFX: Yield loss\n",
      "[Z8K00014817]\n",
      "\n",
      "\n",
      "2 ) Failure mode FM of the focus element: NSOP / NSOG/ lifted ball\n",
      "\n",
      "\n",
      "2 ) Current preventive control PC for FC: 1) Chlorine free glove is compulsory as specified in OJTI (Z8K00004418) (P)\n",
      "\n",
      "\n",
      "2 ) Current detection control DC for FC or FM: i) Visual Inspection - During 2hourly inspection / 3rd Optical inspection / Every change/replacement capillary / Every change device / Safe Release (D)\n",
      "\n",
      "ii) To check plasma effectiveness - Acetone test checking every Start-up after repair/ PM Z8K00056753 (D)\n",
      "\n",
      "iii) Ball Shear Test - 1x/day/mc / Every Change or Replacement capillary / Every Change Device / Safe Release (D), Ball Pull Test - 15mc/day / Every Change or Replacement capillary / Every Change Device / Safe Release (D)\n",
      "\n",
      "\n",
      "2 ) subject: Unit Process FMEA for QFP+ Wire Bond Copper Wire\n",
      "\n",
      "\n",
      "2 ) package: QFP+\n",
      "\n",
      "\n",
      "2 ) location: Manufacturing-Melaka\n",
      "\n",
      "\n",
      "2 ) fmeaIid: Z8K00014817\n",
      "\n",
      "\n",
      "2 ) fmeaidx_id: Z8K00014817_60\n",
      "\n",
      "\n",
      "\n",
      "----------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_docs, filtered_scores, n_docs = tf_idf_Filtering(db, 50, query, 2)\n",
    "# db = FAISS.from_documents(filtered_docs, embeddings)\n",
    "# filtered_docs = db.similarity_search(preprocess_query, k = 10)\n",
    "retrieve_docs = []\n",
    "i=1\n",
    "for doc, score in zip(filtered_docs, filtered_scores):\n",
    "    print(\"Document Metadata:\")\n",
    "    print(f\"Similarity Score: {score}\")\n",
    "    print(\"\\n\")\n",
    "    temp = []\n",
    "    \n",
    "    for key, value in doc.metadata.items():\n",
    "        temp.append(f\"{key}: {value} \")\n",
    "        print(i,\")\",f\"{key}: {value}\")\n",
    "\n",
    "        print(\"\\n\")\n",
    "       \n",
    "    retrieve_docs.append(' '.join(temp))\n",
    "    print(\"\\n----------------------------\\n\\n\")\n",
    "    i+=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['question: Failure cause of the work element : \\n50.13.1.7 Destroy the wire for reject unit\\n[Method_Z8K00014817]\\n\\n  Function of the process step and product characteristic: 50.0 Bonding station  Failure effect FE to next higher level element and/or end user: Customer: Electrical/Functionality/Reliability Failure\\n[Z8K00014817]\\nIFX: Yield loss\\n[Z8K00014817]  Failure mode FM of the focus element: Stray Wire  Current preventive control PC for FC: i) No destroy wire is require, use reject slip to identify reject unit slot, ink dot on reject die , and punch the lead during Dias inspection (P)\\n\\niii) New design of Stray wire vacuum suction bin (P)  Current detection control DC for FC or FM: ii) Visual Inspection - During 2hourly inspection / 3rd Optical inspection / Every change/replacement capillary / Every change device / Safe Release (D)  subject: Unit Process FMEA for QFP+ Wire Bond Copper Wire  package: QFP+  location: Manufacturing-Melaka  fmeaIid: Z8K00014817  fmeaidx_id: Z8K00014817_77 ',\n",
       " 'question: Failure cause of the work element : \\n50.1.7.3 Die Surface Contamination\\n[Material_Z8K00014817]\\n\\n  Function of the process step and product characteristic: 50.0 Bonding station  Failure effect FE to next higher level element and/or end user: Customer: Electrical/Functionality/Reliability Failure\\n[Z8K00014817]\\nIFX: Yield loss\\n[Z8K00014817]  Failure mode FM of the focus element: NSOP / NSOG/ lifted ball  Current preventive control PC for FC: 1) Chlorine free glove is compulsory as specified in OJTI (Z8K00004418) (P)  Current detection control DC for FC or FM: i) Visual Inspection - During 2hourly inspection / 3rd Optical inspection / Every change/replacement capillary / Every change device / Safe Release (D)\\n\\nii) To check plasma effectiveness - Acetone test checking every Start-up after repair/ PM Z8K00056753 (D)\\n\\niii) Ball Shear Test - 1x/day/mc / Every Change or Replacement capillary / Every Change Device / Safe Release (D), Ball Pull Test - 15mc/day / Every Change or Replacement capillary / Every Change Device / Safe Release (D)  subject: Unit Process FMEA for QFP+ Wire Bond Copper Wire  package: QFP+  location: Manufacturing-Melaka  fmeaIid: Z8K00014817  fmeaidx_id: Z8K00014817_60 ']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieve_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "look best ai tool technique 2024\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import contractions\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# 下载必要的NLTK数据\n",
    "import nltk\n",
    "\n",
    "\n",
    "# 定义函数将混合大小写转换为小写\n",
    "def convert_mixed_case_to_lower(text):\n",
    "    if isinstance(text, str):\n",
    "        return text.lower()\n",
    "    return text\n",
    "\n",
    "# 去除标点符号\n",
    "def remove_punctuation(text):\n",
    "    text = re.sub(r'/', ' or ', text)\n",
    "    text = re.sub(r'&amp;', ' and ', text)\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    return text\n",
    "\n",
    "# 扩展缩写\n",
    "def expand_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "# 去除停用词\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_text)\n",
    "\n",
    "# 词干提取\n",
    "def stem_words(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    word_tokens = word_tokenize(text)\n",
    "    stemmed_text = [stemmer.stem(word) for word in word_tokens]\n",
    "    return ' '.join(stemmed_text)\n",
    "\n",
    "# 词形还原\n",
    "def lemmatize_words(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    word_tokens = word_tokenize(text)\n",
    "    lemmatized_text = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in word_tokens]\n",
    "    return ' '.join(lemmatized_text)\n",
    "\n",
    "# 获取词性\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "# 综合预处理函数\n",
    "def preprocess_input(text):\n",
    "    text = convert_mixed_case_to_lower(text)\n",
    "    text = expand_contractions(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = lemmatize_words(text)\n",
    "    return text\n",
    "\n",
    "# 示例使用\n",
    "user_input = \"I'm looking for the best AI tools & techniques in 2024!\"\n",
    "processed_input = preprocess_input(user_input)\n",
    "print(processed_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
